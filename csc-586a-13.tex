\documentclass[paper=letter, fontsize=11pt]{scrartcl}
\usepackage{comment}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{acronym}
\usepackage{listings}
\usepackage{sectsty}
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\allsectionsfont{\centering \normalfont\scshape}
\fancyhead{}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{13.6pt}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{	
\normalfont \normalsize 
\textsc{University of Victoria, Computer Science Department} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge CSC-586A Self-Adaptive Systems\
\horrule{2pt} \\[0.5cm]
}

\author{Tory Borsboom, Sean Debroni, Stephan Heinemann}
\date{\normalsize\today}

\begin{document}
\maketitle 

\section{Part III -- Sensor APIs}
\label{part3}

\begin{comment}
iOS
android
arduino
pixhawk / ardupilot / nuttx
- AP\_RangeFinder (maxsonar i$2$c, state object)
- AP\_OpticalFlow (frontend, backend, state object)
- AP\_AHRS (attitude, heading reference system) uses the optflow object
- sensors.pde uses the optical flow state to supply the EKF and the logger
(update\_opticalflow) - Log.pde writes log messages about the current optical flow state
windows fsx sdk (real hardware interface) -> garmin sdk
- sensors.pde uses the sonar sensor to determine the sonar height (read\_sonar)
- log.pde logs sonar sensor in control\_tuning and optical flow in
log\_write\_optflow
- GCS\_Common.cpp sends the mavlink message to the GCS and uses the sonar height
above ground level (HAGL) - data is used directly and changing logged data is
not effective (mavlink\_msg\_optical\_flow\_send)
- TODO: order longer Iris legs
\end{comment}

\subsection{Android Sensor API}
\label{android_gyroscope_api}
\par
Android devices have a wide variety of sensors available to them, of both the hardware and software variety. For Android, hardware sensors are physical components build into the Android device, while software sensors are built on top of one or more hardware sensors, and interpret their data in a more useful fashion. Sensors for Android devices usually fall into one of these three categories: motion sensors, environmental sensors, and position sensors.\\

\par
The Sensor API for Android consists of four main classes and interfaces, {\em SensorManager}, {\em Sensor}, {\em SensorEventListener}, and {\em SensorEvent}.\\

\par
The {\em SensorManager} class is used to access a device's sensors. The following code is how you would get an instance of this class.\\

\begin{lstlisting} [language=java]
SensorManager sm = (SensorManager) getSystemService(Context.SENSOR_SERVICE);
\end{lstlisting}

\par
The {\em Sensor} class represents one of the device's sensors. Each Sensor has a unique type, which is used to get an instance of that sensor. For example, to get the Gyroscope sensor, you would use the following code.\\

\begin{lstlisting} [language=java]
Sensor s =  sm.getDefaultSensor(Sensor.TYPE_GYROSCOPE);
\end{lstlisting}

\par
{\em SensorEventListener} is an interface that contains two callback functions for the user to implement, onSensorChanged(SensorEvent event), and onAccuracyChanged(Sensor sensor, int accuracy). Say I have implemented {\em SensorEventListener} in a class called {\em MySensorEventListener}, for the Gyroscope sensor. To register a listener, the code would look like this:\\

\begin{lstlisting} [language=java]
MySensorEventListener msel = new MySensorEventListener();
sm.registerListener(msel, s, SensorManager.SENSOR_DELAY_UI);
\end{lstlisting}

\par
SENSOR\_DELAY\_UI is one of four different delay settings, measured in microseconds, that controls how often onSensorChanged (the callback function) is called. These are, in order of fastest to slowest, SENSOR\_DELAY\_FASTEST, SENSOR\_DELAY\_GAME, SENSOR\_DELAY\_NORMAL, and SENSOR\_DELAY\_UI. This timing is just a hint to the system, events may be received faster or slower the the specified rate. From Android 2.3 onwards, you can specify any delay in microseconds, not just these four constants.\\

\par
A {\em SensorEvent} consists of four fields - the accuracy of the event, which sensor generated the event, a timestamp indicating the time in nanoseconds that the event occurred at, and a float array of values, whose length and contents depends upon the type of sensor being monitored. For the Gyroscope sensor, this float array contains three values, the angular speed around the x, y, and z axis' in radians/second.\\

\par
The Android Sensor API consists of these four classes and interfaces - it doesn't matter what sensor you are monitoring, the only thing that changes is the implementation of onSensorChanged and onAccuracyChanged. This is a very nice generic interface for Sensors.\footnote{All information is sourced from the Android Documentation: developer.android.com/guide/topics/sensors/sensors\_overview.html}

\subsection{iOS Magnetic Compass API}
\label{ios_compass_api}
\par
The {\em iOS}\footnote{https://www.apple.com/ca/ios/what-is/} platform used on
{\em Apple}\footnote{https://www.apple.com} phones and tablets supports a
variety of motion services via an object called {\em CMMotionManager}. This
object is the gateway to raw {\em accelerometer/gyroscope/magnetometer} data, as
well as some processed data such as {\em rotation rate}, {\em attitude},
{\em calibrated magnetic fields}, {\em direction of gravity}, and {\em the
direction of acceleration of the device}.
Of particular interest here, however, is the magnetometer and how the device
handles the data collected by this sensor.

\par
The magnetometer in the device collects magnetic field data, such as strength
and polarity, which is accessible via the {\em CMMotionManager} object discussed
above by either setting an interval at which updates can be pushed, or by
polling the sensor whenever an update is required. If requesting updates at
intervals the {\em accelerometerUpdateInterval} property must be specified and
the {\em startMagnetometerUpdatesToQueue:withHandler:} method is called which
requires a {\em CMMagnetometerHandler} type object be passed into it. This
method will add data to a queue which can then be processed as needed. The
purpose of the Handler is simply error handling. The alternative to requesting
repeated information at intervals is periodic sampling of the data which is
performed by calling {\em startMagnetometerUpdates} and then simply reading the
{\em magnetomerData} property. In the case of both of these methods it is
important that the {\em stopMagnetometerUpdates} method is called when the
system no longer needs to process magnetometer data. 

\par
In order to access the data acquired from the most recent polling of the sensors {\em
magnetometerData} must be accessed, this property is a structure of type
{\em CMMagnetometerData}. It contains three int values x, y, and z. These int
values represent directional magnetic field relative to the body of the device
using microteslas as units. With regards to the coordinate system used here The z
axis is positive up through the front of the screen, the y axis is positive up
through the top of the phone, and the x axis is positive to the right of the device
if the screen is facing you. There are also two methods made for simply polling the
sensor to see if it is working or available {\em magnetometerActive} and 
{\em magnetometerAvailable}. {\em magnetometerActive} checks to see if the sensor
is currently taking readings and {\em magnetometerAvailable} checks to see if the sensor is
available for use.

\par
The purpose of having the two separate collection methods is one of resource
management. If the purpose of the app being written is something like a game or
some sort of mapping software, it is unnecessary to acquire data rapidly and
keep it in a queue. Most programs intended for light users will simply need to
query the sensor once per frame rendered on the screen at the very most. The
only time it would be necessary to use the queueing method is if the data needs to be incredibly
precise and with little latency between updates, such as if the device were
being used as a magnetometer in an experiment measuring magnetic flux.

\subsection{NuttX / ArduPilot PX4 Optical Flow and Sonar API}
\label{ardupilot_flow_api}
\lstset{language=C++}
\par
The {\em ArduPilot}\footnote{http://ardupilot.com/} autopilot stack supports a
wide range of sensors to be used with any of the supported vehicle types
including {\em Plane}, {\em Copter} and {\em Rover}. It builds on top of the
{\em NuttX}\footnote{http://nuttx.org/} operating system and requires the
applicable firmware for the underlying hardware such as the
{\em Pixhawk}\footnote{https://pixhawk.org} autopilot module.

\par
The supported sensor types include those to capture data about the vehicle's
{\em environment} as well as those related to its own {\em capabilities} in
order to establish an adequate {\em situational awareness}.
Examples of supported environment sensors gather {\em inertial} (attitude and
acceleration), {\em barometric} (altitude and airspeed), {\em magnetic}
(direction), {\em range} (distance), {\em optical} (orientation and recognition)
and {\em satellite} (position) data. Sensors that capture the vehicle's
capability state include a {\em battery monitor} (power consumption and
endurance) and a {\em performance monitor} (scheduler latency). Environment
sensors themselves may provide {\em health} and {\em quality} monitoring
fuctions. Furthermore, all information that is provided by the {\em NuttX}
operating system about its {\em Pixhawk} platform including the available
{\em memory} and {\em filesystem} space can be instrumented to enhance the
vehicle's capability state.

\par
The {ArduPilot} \ac{API} separates sensor {\em frontends} (interfaces) from
their actual {\em backends} (implementations) in order to easily support
different sensors of the same class. These and other \ac{HAL} modules can be
found in the \texttt{libraries}
directory\footnote{https://github.com/diydrones/ardupilot} and are, hence,
separate from any concrete vehicle-related software. The {\em Copter} vehicle
employs the sensor \ac{API} through its \texttt{sensors.pde} arduino sketch
file which represents an even higher level
\ac{API}\footnote{http://www.arduino.cc/en/pmwiki.php?n=Reference/HomePage}
to the actual vehicle functions such as different autoflight modes.

\par
As a representative example, the structure of the {\em PX$4$ Optical Flow /
Sonar}\footnote{https://pixhawk.org/modules/px4flow} sensor interface shall be
explained. It is connected to the {\em Pixhawk} autopilot module as shown in
Figure \ref{fig:pixhawk_px4_flow_connection}.

\begin{figure}[h]
	\centering
    \includegraphics[width=400px]{graphics/PixhawkPX4Flow.png}
	\caption{Pixhawk PX4 Flow Connection}
	\label{fig:pixhawk_px4_flow_connection}
\end{figure}

\par
This sensor allows to capture the optical flow, that is, the rate of
translational and rotational changes in a captured image, in order to establish
positional information. The optical flow camera is mounted on the bottom of the
{\em Iris}\footnote{http://3drobotics.com/iris/} quadcopter body pointing
downwards and sampling the floor pattern that is being overflown. Since the
aircraft can be tilted (pitch and bank) and flown at different altitudes, a
range finder (sonar sensor) on the same sensor board is employed to relate the
optical flow information to the measured ground distances accordingly. Assuming
the actual attitude of the aircraft based on inertial sensors is known, an
actual ground distance can be derived from the sonar slant range readings.

\par
Figure \ref{fig:optical_flow_api} shows a small extract of the optical flow
sensor frontend \ac{API} in the \texttt{libraries/} \texttt{AP\_OpticalFlow}
directory possessing the sensor state data structure as well as methods that determine
flow and body rates in a two-dimensional image. The interface itself
furthermore consists of methods that determine the health of the sensor and the
quality of the data being captured by it.

\begin{figure}[h]
	\begin{lstlisting}[basicstyle=\scriptsize]
class OpticalFlow { //...
public: //...
	bool enabled() const { return _enabled; }
	bool healthy() const { return backend != NULL && _flags.healthy; }
	uint8_t quality() const { return _state.surface_quality; }
	const Vector2f& flowRate() const { return _state.flowRate; }
	const Vector2f& bodyRate() const { return _state.bodyRate; }
	uint8_t device_id() const { return _state.device_id; }
    //...
    struct OpticalFlow_state {
    	uint8_t device_id;
    	uint8_t  surface_quality;
    	Vector2f flowRate;
    	Vector2f bodyRate;
    };
private: //...
};
	\end{lstlisting}
	\caption{Optical Flow Sensor \ac{API}}
	\label{fig:optical_flow_api}
\end{figure}

\par
Figure \ref{fig:range_finder_api} shows an \ac{API} extract of the range finder
component of the optical flow sensor board. On the {\em PX$4$ Optical Flow}
board, a sonar sensor is employed to obtain range in order to relate optical
flow to ground distance. This concrete implementation is abstracted by the
more generic range finder \ac{API} in the \texttt{libraries/AP\_RangeFinder}
directory.

\begin{figure}[h]
	\begin{lstlisting}[basicstyle=\scriptsize]
class RangeFinder {
public: //...
	enum RangeFinder_Status {
		RangeFinder_NotConnected = 0,
		RangeFinder_NoData,
		RangeFinder_OutOfRangeLow,
		RangeFinder_OutOfRangeHigh,
		RangeFinder_Good
	};
	
	struct RangeFinder_State {
		uint8_t instance;
		uint16_t distance_cm;
		uint16_t voltage_mv;
		enum RangeFinder_Status status;
		uint8_t range_valid_count;
		bool pre_arm_check;
		uint16_t pre_arm_distance_min;
		uint16_t pre_arm_distance_max;
	};
	//...
	uint16_t distance_cm() const { return distance_cm(primary_instance); }
	uint16_t voltage_mv() const { return voltage_mv(primary_instance); }
	int16_t ground_clearance_cm() const { return _ground_clearance_cm[primary_instance]; }
	RangeFinder_Status status(void) const { return status(primary_instance);
	//...
private: //...
};
	\end{lstlisting}
	\caption{Range Finder \ac{API}}
	\label{fig:range_finder_api}
\end{figure}

\par
The \texttt{sensors.pde} arduino sketch file uses the above \ac{API} in order
to provide the {\em Copter} vehicle with necessary sensor data to perform its
functions such as automatic flight modes. The overall static structure is shown
in Figure \ref{fig:of_sensor_board_api}.

\begin{figure}[h]
	\centering
    \includegraphics[width=400px]{graphics/OpticalFlowDiagram.png}
	\caption{PX4 Optical Flow Sensor Board \ac{API}}
	\label{fig:of_sensor_board_api}
\end{figure}

\subsection{Application Example}
\par
A small application example makes use of the {\em PX$4$ Optical Flow} board
connected to the {\em Iris} {\em Pixhawk} autopilot module. The application
realizes a flight envelope warning alert. The alert is triggered whenever
translational flow or rotational body rates exceed a certain limit. It is
also triggered when the \ac{AHRS}\footnote{
http://en.wikipedia.org/wiki/Attitude\_and\_heading\_reference\_system}
which fuses different sensor inputs for the current height of the aircraft
including the sonar distance reports a height above ground that exceeds a
certain threshold.

\par
The alert consist a series of high pitch tones whenever the normal envelope
is violated and a long low pitch tone as soon as the normal flight envelope
is re-established. Furthermore, the aircraft status \ac{LED} continues to
flash red as long as the normal envelope is violated.

\subsection{Appliction in the Cloud}
\par
The optical flow sensor can be used to establish position and, hence, to
support higher-level autopilot modes such as loitering and following a
sequence of legs between waypoints. It can be fused with other sensor data
and provide a higher degree of redundancy. The tracks and sensor data of
small \acp{UAV} can be shared in the cloud. Such an existing project is
called {\em Dronshare}\footnote{http://www.droneshare.com/}. Having access
to and being able to mine shared mission data would enable the more
efficient planning and execution of coordinated missions that involve
multiple \acp{UAV}.

\clearpage
\label{appendix}
\input{csc-586a-13-appendix}

\clearpage
\label{bibliography}
\bibliographystyle{plain}
\bibliography{csc-586a-13}

\end{document}